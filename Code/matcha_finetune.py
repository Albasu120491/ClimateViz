# -*- coding: utf-8 -*-
"""Matcha finetune.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I7CsMbEfptgawGR7A0-J59tpkS_LMabE
"""

import os
import re
import gc
import torch
import pandas as pd
from PIL import Image
from torch.utils.data import Dataset, DataLoader
from transformers import AutoProcessor, Pix2StructForConditionalGeneration, Adafactor
from sklearn.metrics import accuracy_score, f1_score
import unicodedata



MODEL_NAME = "google/matcha-base"

CHECKPOINT_DIR = "./matcha_manual_ckpts"
os.makedirs(CHECKPOINT_DIR, exist_ok=True)

if debug_mode:
    BATCH_SIZE = 2
    MAX_STEPS = 2000
    SAVE_EVERY = 250
    SAMPLES_PER_LABEL = 2000
    GRAD_ACCUM_STEPS = 2
else:
    BATCH_SIZE = 2
    MAX_STEPS = 5000
    SAVE_EVERY = 200
    SAMPLES_PER_LABEL = 1000
    GRAD_ACCUM_STEPS = 2

MAX_EPOCHS = 10
MAX_PATCHES = 512
MAX_LENGTH = 8
VAL_MAX_LENGTH = 16
ALLOWED_LABELS = ["support", "refute", "nei"]
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


processor = AutoProcessor.from_pretrained(MODEL_NAME)
processor.image_processor.max_patches = MAX_PATCHES
model = Pix2StructForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)
model.config.decoder_start_token_id = processor.tokenizer.pad_token_id


def normalize_filename(name):
    name = unicodedata.normalize('NFKD', name).encode('ascii', 'ignore').decode('utf-8')
    name = re.sub(r'[^\w\s-]', '', name).lower().strip()
    return re.sub(r'\s+', '_', name)

def get_image_index(image_dir):
    return {
        os.path.splitext(f)[0].lower(): os.path.join(image_dir, f)
        for f in os.listdir(image_dir)
        if f.lower().endswith(('.png', '.jpg', '.jpeg'))
    }

def match_decoding(pred):
    pred = pred.strip().lower()
    pred = re.sub(r'[^\w]', '', pred)
    if pred.startswith("support"):
        return "support"
    elif pred.startswith("refute"):
        return "refute"
    elif pred.startswith("nei") or pred == "notenoughinfo":
        return "nei"
    else:
        return "nei"

def get_prompt(fact):
    return f"Does the chart support the claim: '{fact}'? Answer support, refute, or NEI."


class ClimateVizDataset(Dataset):
    def __init__(self, dataframe, processor):
        self.data = dataframe.reset_index(drop=True)
        self.processor = processor

    def __len__(self): return len(self.data)

    def __getitem__(self, idx):
        row = self.data.iloc[idx]
        image = Image.open(row['image_path']).convert("RGB")
        prompt = get_prompt(row['fact'])
        label = row['label'].strip().lower()
        inputs = processor(images=image, text=prompt, return_tensors="pt", max_length=256, truncation=True)
        labels = processor.tokenizer(label, return_tensors="pt", max_length=MAX_LENGTH, padding="max_length", truncation=True)
        labels.input_ids[labels.input_ids == processor.tokenizer.pad_token_id] = -100
        return {
            'flattened_patches': inputs['flattened_patches'].squeeze(),
            'attention_mask': inputs['attention_mask'].squeeze(),
            'labels': labels.input_ids.squeeze(),
            'raw_label': label,
            'image_path': row['image_path'],
            'fact': row['fact']
        }

def collate_fn(batch):
    return {
        "flattened_patches": torch.stack([b["flattened_patches"] for b in batch]),
        "attention_mask": torch.stack([b["attention_mask"] for b in batch]),
        "labels": torch.nn.utils.rnn.pad_sequence(
            [b["labels"] for b in batch], batch_first=True, padding_value=-100
        ),
        "raw_labels": [b["raw_label"] for b in batch],
        "image_paths": [b["image_path"] for b in batch],
        "facts": [b["fact"] for b in batch]
    }


df = pd.read_csv(CSV_PATH).dropna(subset=["annotated_title", "fact", "label"])
image_index = get_image_index(IMAGE_DIR)
df["image_path"] = df["annotated_title"].apply(lambda x: image_index.get(normalize_filename(x), None))
df = df[df["image_path"].notnull()]
df = df.groupby("label", group_keys=False).apply(lambda g: g.sample(n=SAMPLES_PER_LABEL, random_state=42)).reset_index(drop=True)

df_train = df.sample(frac=0.85, random_state=42)
df_val = df.drop(df_train.index)

train_loader = DataLoader(ClimateVizDataset(df_train, processor), batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)
val_loader = DataLoader(ClimateVizDataset(df_val, processor), batch_size=1, shuffle=False, collate_fn=collate_fn)


optimizer = Adafactor(
    model.parameters(),
    scale_parameter=True,
    relative_step=True,
    warmup_init=True
)



step = 0
accum_steps = 0
best_val_loss = float("inf")

for epoch in range(MAX_EPOCHS):
    for batch in train_loader:
        model.train()
        step += 1
        accum_steps += 1

        batch_tensors = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}
        outputs = model(**batch_tensors)
        loss = outputs.loss / GRAD_ACCUM_STEPS
        loss.backward()

        if accum_steps % GRAD_ACCUM_STEPS == 0:
            optimizer.step()
            optimizer.zero_grad()
            accum_steps = 0

        if step % 20 == 0:
            print(f"Step {step}: loss = {loss.item() * GRAD_ACCUM_STEPS:.4f}")

        del batch_tensors, outputs, loss
        torch.cuda.empty_cache()
        gc.collect()

        if step % SAVE_EVERY == 0:
            model.eval()
            val_losses, val_preds, val_labels = [], [], []

            with torch.no_grad():
                for val_batch in val_loader:
                    image = Image.open(val_batch["image_paths"][0]).convert("RGB")
                    fact = val_batch["facts"][0]
                    prompt = get_prompt(fact)
                    inputs = processor(images=image, text=prompt, return_tensors="pt").to(device)

                    outputs = model(
                        flattened_patches=inputs["flattened_patches"],
                        attention_mask=inputs["attention_mask"],
                        labels=val_batch["labels"].to(device)
                    )
                    val_losses.append(outputs.loss.item())

                    generated_ids = model.generate(
                        flattened_patches=inputs["flattened_patches"],
                        attention_mask=inputs["attention_mask"],
                        max_length=VAL_MAX_LENGTH,
                        num_beams=1,
                        do_sample=False,
                    )

                    decoded = processor.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
                    preds = [match_decoding(p) for p in decoded]

                    val_preds.extend(preds)
                    val_labels.extend(val_batch["raw_labels"])

                    del outputs, inputs, generated_ids
                    torch.cuda.empty_cache()
                    gc.collect()

            acc = accuracy_score(val_labels, val_preds)
            f1 = f1_score(val_labels, val_preds, average="macro")
            val_loss = sum(val_losses) / len(val_losses)
            print(f"\n>>> Step {step} | Val loss: {val_loss:.4f} | Acc: {acc:.3f} | Macro F1: {f1:.3f}")

            if val_loss < best_val_loss:
                ckpt_path = os.path.join(CHECKPOINT_DIR, f"best_model_step{step}_val{val_loss:.4f}.pt")
                torch.save(model.state_dict(), ckpt_path)
                print(f"Saved new best model: {ckpt_path}")
                best_val_loss = val_loss

            model.train()

        if step >= MAX_STEPS:
            break
    if step >= MAX_STEPS:
        break