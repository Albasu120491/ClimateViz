# -*- coding: utf-8 -*-
"""Knowledge graph.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PYSJVZZ7fYrwIK84pd5jkrP_mE5FarvG
"""

import os
import pandas as pd
import json
import time
import re
from tqdm import tqdm
import openai


openai.api_key = ""


input_csv_path = ""
output_json_dir = ""
os.makedirs(output_json_dir, exist_ok=True)


df = pd.read_csv(input_csv_path)


grouped = list(df.groupby('classification_id'))


BATCH_SIZE = 400
BATCH_START_INDEX = 2400


batch_grouped = grouped[BATCH_START_INDEX: BATCH_START_INDEX + BATCH_SIZE]


MODEL_NAME = "gpt-4o"
TEMPERATURE = 0.0
MAX_RETRIES = 3


def build_prompt(title, chart_type, source, facts_list):
    instruction = """
You are an expert in information extraction and knowledge graph construction for scientific statistical data.

Given the following metadata and factual sentences extracted from a scientific chart, create a structured JSON following this enriched schema:

{
  "title": "<chart title>",
  "type": "<chart type>",
  "source": "<source>",
  "facts": [
    {
      "subject": "<main entity>",
      "subject_type": "<e.g., Region, Indicator, Location>",
      "attribute": "<measured attribute>",
      "attribute_type": "<e.g., Physical Measurement, Anomaly, Flux>",
      "value": "<numeric value or qualitative trend>",
      "value_range": {
        "min": "<min value if range>",
        "max": "<max value if range>"
      },
      "unit": "<unit if applicable>",
      "time_range": "<years or months>",
      "temporal_granularity": "<seasonal, yearly, monthly, etc.>",
      "relation": "<relation verb>",
      "trend": "<increasing, decreasing, stable, fluctuating>",
      "reference_frame": "<baseline comparison if any>",
      "uncertainty": "<uncertainty or confidence band>"
    }
  ]
}

Be faithful and conservative. If information is missing, fill it as null.
"""

    facts_text = "\n".join([f"- {fact}" for fact in facts_list])

    full_prompt = (
        instruction
        + f"\n\nChart Title: {title}\nChart Type: {chart_type}\nSource: {source}\nFacts:\n{facts_text}\n\nOutput only valid JSON without any explanation."
    )

    return full_prompt


def query_gpt(prompt):
    retries = 0
    while retries < MAX_RETRIES:
        try:
            response = openai.chat.completions.create(
                model=MODEL_NAME,
                messages=[{"role": "user", "content": prompt}],
                temperature=TEMPERATURE
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"Error contacting OpenAI: {e}")
            retries += 1
            time.sleep(2 ** retries)
    raise Exception("Max retries reached when querying GPT.")

# Safe JSON parsing
def safe_json_parse(gpt_response):
    try:
        return json.loads(gpt_response)
    except json.JSONDecodeError:
        try:
            json_text = re.search(r"\{.*\}", gpt_response, re.DOTALL).group()
            return json.loads(json_text)
        except Exception as e:
            raise ValueError(f"Failed to extract JSON from GPT response: {e}")

def slugify(text):
    if not isinstance(text, str):
        return None
    text = text.lower()
    text = re.sub(r'[^\w\s-]', '', text)
    text = re.sub(r'[\s_-]+', '_', text)
    text = re.sub(r'^-+|-+$', '', text)
    return text.strip('_')

# Main processing loop
success_count = 0
failure_count = 0

for classification_id, group in tqdm(batch_grouped, desc=f"Processing charts {BATCH_START_INDEX} to {BATCH_START_INDEX + BATCH_SIZE - 1}"):
    title = group['annotated_title'].iloc[0]
    chart_type = group['type'].iloc[0]
    source = group['source'].iloc[0]
    facts_list = group['facts'].dropna().tolist()

    if not facts_list:
        continue  # Skip empty charts

    prompt = build_prompt(title, chart_type, source, facts_list)

    try:
        gpt_response = query_gpt(prompt)
        parsed_json = safe_json_parse(gpt_response)

        safe_title = slugify(title)
        if safe_title:
            filename = f"{safe_title}.json"
        else:
            filename = f"{classification_id}.json"

        output_path = os.path.join(output_json_dir, filename)

        with open(output_path, "w") as f:
            json.dump(parsed_json, f, indent=2)

        success_count += 1

    except Exception as e:
        print(f"Failed to process chart {classification_id} (title={title}): {e}")

        fallback_name = str(classification_id) if pd.isna(title) else slugify(title)
        bad_output_filename = f"{fallback_name}_bad.txt"
        bad_output_path = os.path.join(output_json_dir, bad_output_filename)

        try:
            with open(bad_output_path, "w") as f:
                f.write(gpt_response if 'gpt_response' in locals() else "No GPT output.")
        except Exception as file_error:
            print(f"Also failed to save bad output for chart {classification_id}: {file_error}")

        failure_count += 1

print(f"\n Finished processing batch {BATCH_START_INDEX} to {BATCH_START_INDEX + BATCH_SIZE - 1}.")
print(f"Successful charts: {success_count}")
print(f"Failed charts: {failure_count}")