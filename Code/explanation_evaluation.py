# -*- coding: utf-8 -*-
"""Explanation evaluation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kbjdi6zKoeiqtHojyM0EdOkyjkh7sXAv
"""

import os
import json
import pandas as pd
from tqdm import tqdm
import difflib
import torch
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from nltk.translate.meteor_score import meteor_score
from rouge_score import rouge_scorer
from bert_score import score as bert_score



# Paths
csv_dir = ""
gold_json_dir = ""

csv_files = [

]


available_files = os.listdir(gold_json_dir)
available_titles = [os.path.splitext(f)[0] for f in available_files if f.endswith(".json")]


rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
smooth_fn = SmoothingFunction().method1

def normalize(text):
    return str(text).strip().lower()

def extract_triplets_from_facts(facts):
    triplets = []
    for fact in facts:
        h = normalize(fact.get("subject", ""))
        r = normalize(fact.get("relation", ""))
        t = normalize(fact.get("value")) or normalize(fact.get("time_range")) or normalize(fact.get("attribute"))
        if h and r and t:
            triplets.append((h, r, t))
    return triplets

def fuzzy_match_title(query, candidates):
    norm_query = ''.join(e.lower() for e in query if e.isalnum())
    norm_candidates = [''.join(e.lower() for e in c if e.isalnum()) for c in candidates]
    match = difflib.get_close_matches(norm_query, norm_candidates, n=1, cutoff=0.6)
    if match:
        index = norm_candidates.index(match[0])
        return candidates[index]
    return None


all_results = {}


for csv_file in csv_files:
    print(f"\nðŸ“‚ Evaluating: {csv_file}")
    df = pd.read_csv(os.path.join(csv_dir, csv_file))

    exact_match_f1s, bleu_scores, meteor_scores, rouge_l_scores = [], [], [], []
    bert_preds, bert_golds = [], []

    for _, row in tqdm(df.iterrows(), total=len(df), desc=f"Evaluating {csv_file}"):
        annotated_title = str(row.get("annotated_title", "")).strip()
        pred_expl = row.get("explanation", "")

        if not annotated_title or pd.isna(pred_expl):
            continue

        matched_title = fuzzy_match_title(annotated_title, available_titles)
        if not matched_title:
            continue

        gold_path = os.path.join(gold_json_dir, f"{matched_title}.json")
        try:
            with open(gold_path, "r") as f:
                gold_data = json.load(f)
            gold_triplets = extract_triplets_from_facts(gold_data.get("facts", []))
        except Exception:
            continue

        try:
            pred_blocks = pred_expl.strip().split('\n')
            pred_triplets = []
            for block in pred_blocks:
                parsed = json.loads(block)
                pred_triplets.extend(extract_triplets_from_facts(parsed.get("facts", [])))
        except Exception:
            continue

        if not pred_triplets or not gold_triplets:
            continue

        pred_strs = [' | '.join(t) for t in pred_triplets]
        gold_strs = [' | '.join(t) for t in gold_triplets]
        gold_set, pred_set = set(gold_strs), set(pred_strs)

        # Exact Match F1
        tp = len(gold_set & pred_set)
        precision = tp / len(pred_set) if pred_set else 0
        recall = tp / len(gold_set) if gold_set else 0
        f1_exact = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
        exact_match_f1s.append(f1_exact)

        # BLEU, METEOR, ROUGE-L
        bleu_sample, meteor_sample, rouge_sample = [], [], []
        for pred in pred_strs:
            best_bleu = max(sentence_bleu([g.split()], pred.split(), smoothing_function=smooth_fn) for g in gold_strs)
            best_meteor = max(meteor_score([g.split()], pred.split()) for g in gold_strs)
            best_rouge = max(rouge.score(pred, g)['rougeL'].fmeasure for g in gold_strs)
            bleu_sample.append(best_bleu)
            meteor_sample.append(best_meteor)
            rouge_sample.append(best_rouge)
            for g in gold_strs:
                bert_preds.append(pred)
                bert_golds.append(g)

        bleu_scores.append(sum(bleu_sample) / len(bleu_sample))
        meteor_scores.append(sum(meteor_sample) / len(meteor_sample))
        rouge_l_scores.append(sum(rouge_sample) / len(rouge_sample))

    # BERTScore
    if bert_preds and bert_golds:
        P, R, F1 = bert_score(bert_preds, bert_golds, lang='en', device='cuda' if torch.cuda.is_available() else 'cpu')
        bert_f1_avg = F1.mean().item()
    else:
        bert_f1_avg = None

    all_results[csv_file] = {
        "Exact Match F1": round(sum(exact_match_f1s) / len(exact_match_f1s), 4) if exact_match_f1s else None,
        "BLEU": round(sum(bleu_scores) / len(bleu_scores), 4) if bleu_scores else None,
        "METEOR": round(sum(meteor_scores) / len(meteor_scores), 4) if meteor_scores else None,
        "ROUGE-L": round(sum(rouge_l_scores) / len(rouge_l_scores), 4) if rouge_l_scores else None,
        "BERTScore F1": round(bert_f1_avg, 4) if bert_f1_avg is not None else None
    }

# Print Results
print("\n Evaluation Summary (Triplet-Level)")
for model_name, metrics in all_results.items():
    print(f"\n Model: {model_name}")
    for metric, val in metrics.items():
        print(f"   {metric}: {val if val is not None else 'N/A'}")